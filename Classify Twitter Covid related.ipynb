{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n",
    "                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_textnum(text):\n",
    "    '''\n",
    "    To seperate numbers from the words.\n",
    "    \n",
    "    Input - Word\n",
    "    \n",
    "    Returns - Number seperated list of items.\n",
    "    '''\n",
    "    match = re.match(r\"([a-z]+)([0-9]+)\", text, re.I)\n",
    "    if match:\n",
    "        items = \" \".join(list(match.groups()))\n",
    "    else:\n",
    "        match = re.match(r\"([0-9]+)([a-z]+)\", text, re.I)\n",
    "        if match:\n",
    "            items = \" \".join(list(match.groups()))\n",
    "        else:\n",
    "            return text\n",
    "    return (items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "            \n",
    "    # Special characters\n",
    "    text = re.sub(r\"%20\", \" \", text)\n",
    "    #text = text.replace(r\".\", \" \")\n",
    "    text = text.replace(r\"@\", \" \")\n",
    "    text = text.replace(r\"#\", \" \")\n",
    "    #text = text.replace(r\":\", \" \")\n",
    "    text = text.replace(r\"'\", \" \")\n",
    "    text = text.replace(r\"\\x89û_\", \" \")\n",
    "    text = text.replace(r\"??????\", \" \")\n",
    "    text = text.replace(r\"\\x89ûò\", \" \")\n",
    "    text = text.replace(r\"16yr\", \"16 year\")\n",
    "    text = text.replace(r\"re\\x89û_\", \" \")\n",
    "    \n",
    "    text = text.replace(r\"mh370\", \" \")\n",
    "    text = text.replace(r\"prebreak\", \"pre break\")\n",
    "    text = re.sub(r\"\\x89û\", \" \", text)\n",
    "    text = re.sub(r\"re\\x89û\", \"re \", text)\n",
    "    text = text.replace(r\"nowplaying\", \"now playing\")\n",
    "    text = re.sub(r\"\\x89ûª\", \"'\", text)\n",
    "    text = re.sub(r\"\\x89û\", \" \", text)\n",
    "    text = re.sub(r\"\\x89ûò\", \" \", text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r\"ªs\", \" \", text)\n",
    "    text = re.sub(r\"ª\", \" \", text)\n",
    "    text = re.sub(r\"\\x9d\", \" \", text)\n",
    "    text = re.sub(r\"ò\", \" \", text)\n",
    "    text = re.sub(r\"ªt\", \" \", text)\n",
    "    text = re.sub(r\"ó\", \" \", text)\n",
    "    text = text.replace(r\"11yearold\", \"11 year old\")\n",
    "    text = re.sub(r\"typhoondevastated\", \"typhoon devastated\", text)\n",
    "    text = re.sub(r\"bestnaijamade\", \"best nijamade\", text)\n",
    "    text = re.sub(r\"gbbo\", \"The Great British Bake Off\", text)\n",
    "    text = re.sub(r\"ï\", \"\", text)\n",
    "    text = re.sub(r\"ïwhen\", \"when\", text)\n",
    "    text = re.sub(r\"selfimage\", \"self image\", text)\n",
    "    text = re.sub(r\"20150805\", \"2015 08 05\", text)\n",
    "    text = re.sub(r\"20150806\", \"2015 08 06\", text)\n",
    "    text = re.sub(r\"subreddits\", \"website for weird public sentiment\", text)\n",
    "    text = re.sub(r\"disea\", \"chinese famous electronic company\", text)\n",
    "    text = re.sub(r\"lmao\", \"funny\", text)\n",
    "    text = text.replace(r\"companyse\", \"company\")\n",
    "    \n",
    "    text = text.replace(r\"worldnews\", \"world news\")\n",
    "    text = text.replace(r\"animalrescue\", \"animal rescue\")\n",
    "    text = text.replace(r\"freakiest\", \"freak\")\n",
    "    \n",
    "    text = text.replace(r\"irandeal\", \"iran deal\")\n",
    "    text = text.replace(r\"directioners\", \"mentor\")\n",
    "    text = text.replace(r\"justinbieber\", \"justin bieber\")\n",
    "    text = text.replace(r\"okwx\", \"okay\")\n",
    "    text = text.replace(r\"trapmusic\", \"trap music\")\n",
    "    text = text.replace(r\"djicemoon\", \"music ice moon\")\n",
    "    text = text.replace(r\"icemoon\", \"ice moon\")\n",
    "    text = text.replace(r\"mtvhottest\", \"tv hottest\")\n",
    "    text = text.replace(r\"rì©union\", \"reunion\")\n",
    "    text = text.replace(r\"abcnews\", \"abc news\")\n",
    "    text = text.replace(r\"tubestrike\", \"tube strike\")\n",
    "    text = text.replace(r\"prophetmuhammad\", \"prophet muhammad muslim dharma\")\n",
    "    text = text.replace(r\"chicagoarea\", \"chicago area\")\n",
    "    text = text.replace(r\"yearold\", \"year old\")\n",
    "    text = text.replace(r\"meatloving\", \"meat love\")\n",
    "    text = text.replace(r\"standuser\", \"standard user\")\n",
    "    text = text.replace(r\"pantherattack\", \"panther attack\")\n",
    "    text = text.replace(r\"youngheroesid\", \"young hearos id\")\n",
    "    text = text.replace(r\"idk\", \"i do not know\")\n",
    "    text = text.replace(r\"usagov\", \"united state of america government\")\n",
    "    text = text.replace(r\"injuryi\", \"injury\")\n",
    "    text = text.replace(r\"summerfate\", \"summer fate\")\n",
    "    text = text.replace(r\"kerricktrial\", \"kerrick trial\")\n",
    "    text = text.replace(r\"viralspell\", \"viral spell\")\n",
    "    text = text.replace(r\"collisionno\", \"collision\")\n",
    "    text = text.replace(r\"socialnews\", \"social news\")\n",
    "    text = text.replace(r\"nasahurricane\", \"nasa hurricane\")\n",
    "    text = text.replace(r\"strategicpatience\", \"strategic patience\")\n",
    "    text = text.replace(r\"explosionproof\", \"explosion proof\")\n",
    "    text = text.replace(r\"selfies\", \"photo\")\n",
    "    text = text.replace(r\"selfie\", \"photo\")\n",
    "    text = text.replace(r\"worstsummerjob\", \"worst summer job\")\n",
    "    text = text.replace(r\"realdonaldtrump\", \"real america president\")\n",
    "    text = text.replace(r\"omfg\", \"oh my god\")\n",
    "    text = text.replace(r\"japìn\", \"japan\")\n",
    "    text = text.replace(r\"breakingnews\", \"breaking news\")\n",
    "    \n",
    "    text = \" \".join([split_textnum(word) for word in text.split(\" \")])\n",
    "    \n",
    "    text = \"\".join([c if c not in string.punctuation else \"\" for c in text])\n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    text = text.replace(r\"÷\", \"\")\n",
    "    \n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # text = text.encode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/covid_related_tf_only.csv\", encoding = \"ISO-8859-1\" ,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['text_processed'] = data['tweet'].apply(lambda x : \" \".join([contraction_mapping[word].lower() \n",
    "                    if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\n",
    "# X_test['text_processed'] = X_test['tweet'].apply(lambda x : \" \".join([contraction_mapping[word].lower() \n",
    "#                     if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\n",
    "# X_train['text_processed'] = X_train['text_processed'].apply(lambda x : clean_text(x))\n",
    "# X_test['text_processed'] = X_test['text_processed'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "data['text_processed'] = data['text_processed'].apply(lambda x : \"\".join([lemmatizer.lemmatize(word) \n",
    "                                                                            for word in x]))\n",
    "# X_test['text_processed'] = X_test['text_processed'].apply(lambda x : \"\".join([lemmatizer.lemmatize(word) \n",
    "#                                                                           for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "    return train, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = data[\"text_processed\"].tolist()\n",
    "list_labels = data[\"covid_related\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Bag of Words embeddings\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "#TF-IDF embeddings\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(X_train_counts, open(\"data/train_count.pickle\", \"wb\"))\n",
    "pickle.dump(X_train_tfidf, open(\"data/train_tfidf.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(X_test_counts, open(\"data/test_count.pickle\", \"wb\"))\n",
    "pickle.dump(X_test_tfidf, open(\"data/test_tfidf.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(y_train, open(\"data/y_train.pickle\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"data/y_test.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "pickle.dump(count_vectorizer, open(\"data/count_vectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(tfidf_vectorizer, open(\"data/tfidf_vectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = pickle.load(open(\"data/train_count.pickle\", \"rb\"))\n",
    "X_train_tfidf = pickle.load(open(\"data/train_tfidf.pickle\", \"rb\"))\n",
    "\n",
    "X_test_counts = pickle.load(open(\"data/test_count.pickle\", \"rb\"))\n",
    "X_test_tfidf = pickle.load(open(\"data/test_tfidf.pickle\", \"rb\"))\n",
    "\n",
    "y_train = pickle.load(open(\"data/y_train.pickle\", \"rb\"))\n",
    "y_test = pickle.load(open(\"data/y_test.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2523, 24063)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9108204518430439"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_counts)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904478795085216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816884661117717"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_counts.toarray(), y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_counts.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8164883075703527"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270709472849782"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_counts.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270709472849782"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9207292905271502"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_counts)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.43      0.23      0.30       185\n",
      "        True       0.94      0.98      0.96      2338\n",
      "\n",
      "    accuracy                           0.92      2523\n",
      "   macro avg       0.68      0.60      0.63      2523\n",
      "weighted avg       0.90      0.92      0.91      2523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318271898533492"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.71      0.12      0.20       185\n",
      "        True       0.93      1.00      0.96      2338\n",
      "\n",
      "    accuracy                           0.93      2523\n",
      "   macro avg       0.82      0.56      0.58      2523\n",
      "weighted avg       0.92      0.93      0.91      2523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262782401902497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test_counts)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"data/Data for joining/covid_related_full_ids.csv\", engine=\"python\", index_col=False)\n",
    "df_9999 = pd.read_csv(\"data/Data for joining/tweets_to_label_Batch3_9999.csv\", engine=\"python\", index_col=False)\n",
    "df_url = pd.read_csv(\"data/Data for joining/tweets_url_to_text.csv\", engine=\"python\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.merge(df_url.rename(columns={\"tweet_id\":\"id\"}), df_9999, on=\"id\", how= \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>http_response_code</th>\n",
       "      <th>url_title</th>\n",
       "      <th>url_body</th>\n",
       "      <th>full_text</th>\n",
       "      <th>user_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222863488298565632</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the government: coronavirus who ??? https://t....</td>\n",
       "      <td>“ vaguen. ”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222438588555522048</td>\n",
       "      <td>200</td>\n",
       "      <td>Wuhan Coronavirus Infections | Scientist Warni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This scientist hopes to test coronavirus drugs...</td>\n",
       "      <td>NicoleReloaded🙋🏽‍♀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221951156940754945</td>\n",
       "      <td>200</td>\n",
       "      <td>Coronavirus - Canada has advised to avoid all ...</td>\n",
       "      <td>Canada has advised Canadians not to travel to ...</td>\n",
       "      <td>Coronavirus - Canada has advised to avoid all ...</td>\n",
       "      <td>Skygains.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1222428301920092165</td>\n",
       "      <td>200</td>\n",
       "      <td>Can the coronavirus be contained? Unknowns com...</td>\n",
       "      <td>Some early signs are discouraging: Six countri...</td>\n",
       "      <td>Can the coronavirus be contained? Unknowns com...</td>\n",
       "      <td>Walkirie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1221919177214242818</td>\n",
       "      <td>200</td>\n",
       "      <td>45 Million Chinese Now Under Quarantine As Off...</td>\n",
       "      <td>45 Million Chinese Now Under Quarantine As Off...</td>\n",
       "      <td>New story on NPR: 45 Million Chinese Now Under...</td>\n",
       "      <td>János Medenica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>1237505424829476864</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is what the status quo gets you. Healthca...</td>\n",
       "      <td>Eric Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td>1242687185855246338</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perfect. Now release the funds 😈 https://t.co/...</td>\n",
       "      <td>🕵🏽‍♀️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7151</th>\n",
       "      <td>1244603588317458437</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Humanity is in the gutter! How utterly depress...</td>\n",
       "      <td>Karen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>1239027328136531968</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>She is the reason why I am interested SO MUCH ...</td>\n",
       "      <td>ioo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>1238570530615119872</td>\n",
       "      <td>200</td>\n",
       "      <td>Online: ENC Mecklenburg March Meeting</td>\n",
       "      <td>Email or Phone Password Forgot account? Log In...</td>\n",
       "      <td>Please join us for our Mecklenburg County meet...</td>\n",
       "      <td>Equality NC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7154 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id http_response_code  \\\n",
       "0     1222863488298565632               400    \n",
       "1     1222438588555522048                200   \n",
       "2     1221951156940754945                200   \n",
       "3     1222428301920092165                200   \n",
       "4     1221919177214242818                200   \n",
       "...                   ...                ...   \n",
       "7149  1237505424829476864               400    \n",
       "7150  1242687185855246338               400    \n",
       "7151  1244603588317458437               400    \n",
       "7152  1239027328136531968               400    \n",
       "7153  1238570530615119872                200   \n",
       "\n",
       "                                              url_title  \\\n",
       "0                                                   NaN   \n",
       "1     Wuhan Coronavirus Infections | Scientist Warni...   \n",
       "2     Coronavirus - Canada has advised to avoid all ...   \n",
       "3     Can the coronavirus be contained? Unknowns com...   \n",
       "4     45 Million Chinese Now Under Quarantine As Off...   \n",
       "...                                                 ...   \n",
       "7149                                                NaN   \n",
       "7150                                                NaN   \n",
       "7151                                                NaN   \n",
       "7152                                                NaN   \n",
       "7153              Online: ENC Mecklenburg March Meeting   \n",
       "\n",
       "                                               url_body  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     Canada has advised Canadians not to travel to ...   \n",
       "3     Some early signs are discouraging: Six countri...   \n",
       "4     45 Million Chinese Now Under Quarantine As Off...   \n",
       "...                                                 ...   \n",
       "7149                                                NaN   \n",
       "7150                                                NaN   \n",
       "7151                                                NaN   \n",
       "7152                                                NaN   \n",
       "7153  Email or Phone Password Forgot account? Log In...   \n",
       "\n",
       "                                              full_text           user_name  \n",
       "0     the government: coronavirus who ??? https://t....         “ vaguen. ”  \n",
       "1     This scientist hopes to test coronavirus drugs...  NicoleReloaded🙋🏽‍♀  \n",
       "2     Coronavirus - Canada has advised to avoid all ...        Skygains.com  \n",
       "3     Can the coronavirus be contained? Unknowns com...            Walkirie  \n",
       "4     New story on NPR: 45 Million Chinese Now Under...      János Medenica  \n",
       "...                                                 ...                 ...  \n",
       "7149  This is what the status quo gets you. Healthca...          Eric Smith  \n",
       "7150  Perfect. Now release the funds 😈 https://t.co/...               🕵🏽‍♀️  \n",
       "7151  Humanity is in the gutter! How utterly depress...               Karen  \n",
       "7152  She is the reason why I am interested SO MUCH ...                 ioo  \n",
       "7153  Please join us for our Mecklenburg County meet...         Equality NC  \n",
       "\n",
       "[7154 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>covid_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.221110e+18</td>\n",
       "      <td>60 is alot\\r\\n\\r\\n2nd Wuhan Coronavirus Case C...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.222670e+18</td>\n",
       "      <td>@CDCgov I think we're past the point of pointi...</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.220430e+18</td>\n",
       "      <td>First Bats now snakes ! Get it  right ! https:...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.221420e+18</td>\n",
       "      <td>\"Doctor on coronavirus: �We are more prepared�...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.223270e+18</td>\n",
       "      <td>Trump's Commerce Secretary Gets Dragged for Bo...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10234</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Humanity is in the gutter! How utterly depress...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>NaN</td>\n",
       "      <td>One thing I can say about miss social distanci...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This Morning,Father was giving the communion u...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>NaN</td>\n",
       "      <td>She is the reason why I am interested SO MUCH ...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Please join us for our Mecklenburg County meet...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10239 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                              tweet  \\\n",
       "0      1.221110e+18  60 is alot\\r\\n\\r\\n2nd Wuhan Coronavirus Case C...   \n",
       "1      1.222670e+18  @CDCgov I think we're past the point of pointi...   \n",
       "2      1.220430e+18  First Bats now snakes ! Get it  right ! https:...   \n",
       "3      1.221420e+18  \"Doctor on coronavirus: �We are more prepared�...   \n",
       "4      1.223270e+18  Trump's Commerce Secretary Gets Dragged for Bo...   \n",
       "...             ...                                                ...   \n",
       "10234           NaN  Humanity is in the gutter! How utterly depress...   \n",
       "10235           NaN  One thing I can say about miss social distanci...   \n",
       "10236           NaN  This Morning,Father was giving the communion u...   \n",
       "10237           NaN  She is the reason why I am interested SO MUCH ...   \n",
       "10238           NaN  Please join us for our Mecklenburg County meet...   \n",
       "\n",
       "      covid_related  \n",
       "0              TRUE  \n",
       "1             FALSE  \n",
       "2              TRUE  \n",
       "3              TRUE  \n",
       "4              TRUE  \n",
       "...             ...  \n",
       "10234          TRUE  \n",
       "10235          TRUE  \n",
       "10236          TRUE  \n",
       "10237          TRUE  \n",
       "10238          TRUE  \n",
       "\n",
       "[10239 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
